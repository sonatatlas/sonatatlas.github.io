---
layout: default
title: Compilation
---


[Preprocessor](#preprocessor)
In computer science, a preprocessor is a program that processes its input data to produce output that is used as input to another program.

[Compiler](#compiler)
A compiler is computer software that transforms computer code written in one programming language (the source language) into another programming language (the target language). 

[Linker](#linker)
In computing, a linker or link editor is a computer utility program that takes one or more object files generated by a compiler and combines them into a single executable file, library file, or another 'object' file.

In computer systems a loader is the part of an operating system that is responsible for loading programs and libraries.

[Execution](#execution)
Execution in computer and software engineering is the process by which a computer or virtual machine executes the instructions of a computer program. 

[Runtime System](#runtime-system)
A runtime system, also called run-time system, primarily __implements portions of an execution model.__ This is in contrast to the runtime lifecycle phase of a program, during which the runtime system is in operation. 

[Virtual Machine](#virtual-machine)
In computing, a virtual machine (VM) is an emulation of a computer system. Virtual machines are based on computer architectures and provide functionality of a physical computer. 

#
-

# Preprocessor
In computer science, a preprocessor is __a program that processes its input data to produce output that is used as input to another program.__ The output is said to be a preprocessed form of the input data, which is often used by some subsequent programs like compilers. The amount and kind of processing done depends on the nature of the preprocessor; some preprocessors are only capable of performing relatively simple textual substitutions and macro expansions, while others have the power of full-fledged programming languages.

## Lexical preprocessors
Lexical preprocessors are __the lowest-level of preprocessors as they only require lexical analysis__, that is, they operate on the source text, prior to any parsing, by performing simple substitution of tokenized character sequences for other tokenized character sequences, according to user-defined rules. They typically perform macro substitution, textual inclusion of other files, and conditional compilation or inclusion.

## Syntactic preprocessors
Syntactic preprocessors were introduced with the Lisp family of languages. Their role is __to transform syntax trees according to a number of user-defined rules.__ For some programming languages, the rules are written in the same language as the program (compile-time reflection). This is the case with Lisp and OCaml. Some other languages rely on a fully external language to define the transformations, such as the XSLT preprocessor for XML, or its statically typed counterpart CDuce.

### Customizing syntax
A good example of syntax customization is the existence of two different syntaxes in the Objective Caml programming language. __Programs may be written indifferently using the "normal syntax" or the "revised syntax", and may be pretty-printed with either syntax on demand.__

### Extending a language
The best examples of language extension through macros are found in the Lisp family of languages. While the languages, by themselves, __are simple dynamically typed functional cores__, the standard distributions of Scheme or Common Lisp permit imperative or object-oriented programming, as well as static typing. Almost all of these features are implemented by syntactic preprocessing, although it bears noting that the "macro expansion" phase of compilation is handled by the compiler in Lisp. This can still be considered a form of preprocessing, since it takes place before other phases of compilation.

### Specializing a language
One of the unusual features of the Lisp family of languages is the possibility of __using macros to create an internal DSL__. Typically, in a large Lisp-based project, a module may be written in a variety of such minilanguages, one perhaps using a SQL-based dialect of Lisp, another written in a dialect specialized for GUIs or pretty-printing, etc. Common Lisp's standard library contains an example of this level of syntactic abstraction in the form of the LOOP macro, which implements an Algol-like minilanguage to describe complex iteration, while still enabling the use of standard Lisp operators.

## General purpose preprocessor
Most preprocessors are specific to a particular __data processing task (e.g., compiling the C language).__ A preprocessor may be promoted as being general purpose, meaning that it is not aimed at a specific usage or programming language, and is intended to be used for a wide variety of text processing tasks.

M4 is probably the most well known example of such a general purpose preprocessor, although the C preprocessor is sometimes used in a non-C specific role. Examples:

+ using C preprocessor for JavaScript preprocessing.
+ using C preprocessor for devicetree processing within the Linux kernel.
+ using M4 (see on-article example) or C preprocessor as a template engine, to HTML generation.
+ imake, a make interface using the C preprocessor, written for the X Window System but now deprecated in favour of automake.
+ grompp, a preprocessor for simulation input files for GROMACS (a fast, free, open-source code for some problems in computational chemistry) which calls the system C preprocessor (or other preprocessor as determined by the simulation input file) to parse the topology, using mostly the #define and #include mechanisms to determine the effective topology at grompp run time.
+ using GPP for preprocessing markdown files[5]

# 
-

# Compiler
A compiler is computer software that __transforms computer code written in one programming language (the source language) into another programming language (the target language).__ Compilers are a type of translator that support digital devices, primarily computers. The name compiler is primarily used for programs that translate source code from a high-level programming language to a lower level language (e.g., assembly language, object code, or machine code) to create an executable program.

A compiler is likely to perform many or all of the following operations: preprocessing, lexical analysis, parsing, semantic analysis (syntax-directed translation), conversion of input programs to an intermediate representation, code optimization and code generation. Compilers implement these operations in phases that promote efficient design and correct transformations of source input to target output. Program faults caused by incorrect compiler behavior can be very difficult to track down and work around; therefore, compiler implementers invest significant effort to ensure compiler correctness.

## Just-in-time
In computing, just-in-time (JIT) compilation, (also dynamic translation or run-time compilations), __is a way of executing computer code that involves compilation during execution of a program – at run time – rather than prior to execution.__ Most often, this consists of source code or more commonly bytecode translation to machine code, which is then executed directly. A system implementing a JIT compiler typically continuously analyses the code being executed and identifies parts of the code where the speedup gained from compilation or recompilation would outweigh the overhead of compiling that code.

## Ahead-of-time compilation
In computer science, ahead-of-time (AOT) compilation is the act of compiling a higher-level programming language such as C or C++, or an intermediate representation such as Java bytecode or .NET Framework Common Intermediate Language (CIL) code, __into a native (system-dependent) machine code so that the resulting binary file can execute natively.__

## Transcompilation
A source-to-source compiler, transcompiler or transpiler is __a type of compiler that takes the source code of a program written in one programming language as its input and produces the equivalent source code in another programming language.__ A source-to-source compiler translates between programming languages that operate at approximately the same level of abstraction, while a traditional compiler translates from a higher level programming language to a lower level programming language. For example, a source-to-source compiler may perform a translation of a program from Pascal to C. An automatic parallelizing compiler will frequently take in a high level language program as an input and then transform the code and annotate it with parallel code annotations (e.g., OpenMP) or language constructs (e.g. Fortran's forall statements).

## Recompilation
In computer science, dynamic recompilation (sometimes abbreviated to dynarec or the pseudo-acronym DRC) is __a feature of some emulators and virtual machines, where the system may recompile some part of a program during execution.__ By compiling during execution, the system can tailor the generated code to reflect the program's run-time environment, and potentially produce more efficient code by exploiting information that is not available to a traditional static compiler.

## Hardware Compilers
hardware compilers (also known as syntheses tools) are compilers whose output is __a description of the hardware configuration instead of a sequence of instructions.__

+ The output of these compilers target computer hardware at a very low level, for example a field-programmable gate array (FPGA) or structured application-specific integrated circuit (ASIC).Such compilers are said to be hardware compilers, because the source code they compile effectively controls the final configuration of the hardware and how it operates. The output of the compilation is only an interconnection of transistors or lookup tables.

+ An example of hardware compiler is XST, the Xilinx Synthesis Tool used for configuring FPGAs. Similar tools are available from Altera, Synplicity, Synopsys and other hardware vendors.

# Intermediate representation
An Intermediate representation (IR) is __the data structure or code used internally by a compiler or virtual machine to represent source code.__ An IR is designed to be conducive for further processing, such as optimization and translation. A "good" IR must be accurate – capable of representing the source code without loss of information – and independent of any particular source or target language. An IR may take one of several forms: an in-memory data structure, or a special tuple- or stack-based code readable by the program. In the latter case it is also called an intermediate language.

#
-

# Linker
In computing, a linker or link editor is a computer utility program that takes one or more object files generated by a compiler and combines them into a single executable file, library file, or another 'object' file.

## Dynamic Linking
Many operating system environments allow dynamic linking, __deferring the resolution of some undefined symbols until a program is run.__ That means that the executable code still contains undefined symbols, plus a list of objects or libraries that will provide definitions for these. Loading the program will load these objects/libraries as well, and perform a final linking.

## Static Linking
Static linking is __the result of the linker copying all library routines used in the program into the executable image.__ This may require more disk space and memory than dynamic linking, but is more portable, since it does not require the presence of the library on the system where it runs. Static linking also prevents "DLL Hell", since each program includes exactly the versions of library routines that it requires, with no conflict with other programs. A program using just a few routines from a library does not require the entire library to be installed.

## Relocation
As the compiler has no information on the layout of objects in the final output, it cannot take advantage of shorter or more efficient instructions that __place a requirement on the address of another object__. For example, a jump instruction can reference an absolute address or an offset from the current location, and the offset could be expressed with different lengths depending on the distance to the target. By generating the most conservative instruction (usually the largest relative or absolute variant, depending on platform) and adding relaxation hints, it is possible to substitute shorter or more efficient instructions during the final link. This step can be performed only after all input objects have been read and assigned temporary addresses; the linker relaxation pass subsequently reassigns addresses, which may in turn allow more relaxations to occur. In general, the substituted sequences are shorter, which allows this process to always converge on the best solution given a fixed order of objects; if this is not the case, relaxations can conflict, and the linker needs to weigh the advantages of either option.

## Linkage Editor
In IBM System/360 mainframe environments such as OS/360, including z/OS for the z/Architecture mainframes, this type of program is known as a linkage editor. As the name implies a linkage editor __has the additional capability of allowing the addition, replacement, and/or deletion of individual program sections.__ Operating systems such as OS/360 have format for executable load-modules containing supplementary data about the component sections of a program, so that an individual program section can be replaced, and other parts of the program updated so that relocatable addresses and other references can be corrected by the linkage editor, as part of the process.

# Loader
In computer systems a loader is __the part of an operating system that is responsible for loading programs and libraries.__ It is one of the essential stages in the process of starting a program, as it places programs into memory and prepares them for execution. Loading a program involves reading the contents of the executable file containing the program instructions into memory, and then carrying out other required preparatory tasks to prepare the executable for running. Once loading is complete, the operating system starts the program by passing control to the loaded program code.

## Responsibilities
In Unix, the loader is __the handler for the system call execve().__ The Unix loader's tasks include:
+ validation (permissions, memory requirements etc.);
+ copying the program image from the disk into main memory;
+ copying the command-line arguments on the stack;
+ initializing registers (e.g., the stack pointer);
+ jumping to the program entry point (_start).

In Microsoft Windows 7 and above, the loader is __the LdrInitializeThunk function contained in ntdll.dll,__ that does the following:
+ initialisation of structures in the DLL itself (i.e. critical sections, module lists);
+ validation of executable to load;
+ creation of a heap (via the function RtlCreateHeap);
+ allocation of environment variable block and PATH block;
+ addition of executable and NTDLL to the module list (a doubly-linked list);
+ loading of KERNEL32.DLL to obtain several important functions, for instance BaseThreadInitThunk;
+ loading of executable's imports (i.e. dynamic-link libraries) recursively (check the imports' imports, their imports and so on);
+ in debug mode, raising of system breakpoint;
+ initialisation of DLLs;
+ garbage collection;
+ calling NtContinue on the context parameter given to the loader function (i.e. jumping to RtlUserThreadStart, that will start the executable)

## Relocating loaders
Some operating systems need relocating loaders, ___which adjust addresses (pointers) in the executable to compensate for variations in the address at which loading starts.__ The operating systems that need relocating loaders are those in which a program is not always loaded into the same location in the address space and in which pointers are absolute addresses rather than offsets from the program's base address. Some well-known examples are IBM's OS/360 for their System/360 mainframes, and its descendants, including z/OS for the z/Architecture mainframes.

## OS/360 & Derivatives
In OS/360 and descendant systems, the (privileged) operating system facility is called IEWFETCH, and is __an internal component of the OS Supervisor,__ whereas the (non-privileged) LOADER application can perform many of the same functions, plus those of the Linkage Editor, and is entirely external to the OS Supervisor (although it certainly uses many Supervisor services).


#
-

# Preprocessor
In computer science, a preprocessor is __a program that processes its input data to produce output that is used as input to another program.__ The output is said to be a preprocessed form of the input data, which is often used by some subsequent programs like compilers. The amount and kind of processing done depends on the nature of the preprocessor; some preprocessors are only capable of performing relatively simple textual substitutions and macro expansions, while others have the power of full-fledged programming languages.

## Lexical preprocessors
Lexical preprocessors are __the lowest-level of preprocessors as they only require lexical analysis__, that is, they operate on the source text, prior to any parsing, by performing simple substitution of tokenized character sequences for other tokenized character sequences, according to user-defined rules. They typically perform macro substitution, textual inclusion of other files, and conditional compilation or inclusion.

## Syntactic preprocessors
Syntactic preprocessors were introduced with the Lisp family of languages. Their role is __to transform syntax trees according to a number of user-defined rules.__ For some programming languages, the rules are written in the same language as the program (compile-time reflection). This is the case with Lisp and OCaml. Some other languages rely on a fully external language to define the transformations, such as the XSLT preprocessor for XML, or its statically typed counterpart CDuce.

### Customizing syntax
A good example of syntax customization is the existence of two different syntaxes in the Objective Caml programming language. __Programs may be written indifferently using the "normal syntax" or the "revised syntax", and may be pretty-printed with either syntax on demand.__

### Extending a language
The best examples of language extension through macros are found in the Lisp family of languages. While the languages, by themselves, __are simple dynamically typed functional cores__, the standard distributions of Scheme or Common Lisp permit imperative or object-oriented programming, as well as static typing. Almost all of these features are implemented by syntactic preprocessing, although it bears noting that the "macro expansion" phase of compilation is handled by the compiler in Lisp. This can still be considered a form of preprocessing, since it takes place before other phases of compilation.

### Specializing a language
One of the unusual features of the Lisp family of languages is the possibility of __using macros to create an internal DSL__. Typically, in a large Lisp-based project, a module may be written in a variety of such minilanguages, one perhaps using a SQL-based dialect of Lisp, another written in a dialect specialized for GUIs or pretty-printing, etc. Common Lisp's standard library contains an example of this level of syntactic abstraction in the form of the LOOP macro, which implements an Algol-like minilanguage to describe complex iteration, while still enabling the use of standard Lisp operators.

## General purpose preprocessor
Most preprocessors are specific to a particular __data processing task (e.g., compiling the C language).__ A preprocessor may be promoted as being general purpose, meaning that it is not aimed at a specific usage or programming language, and is intended to be used for a wide variety of text processing tasks.

M4 is probably the most well known example of such a general purpose preprocessor, although the C preprocessor is sometimes used in a non-C specific role. Examples:

+ using C preprocessor for JavaScript preprocessing.[2]
+ using C preprocessor for devicetree processing within the Linux kernel.[3]
+ using M4 (see on-article example) or C preprocessor[4] as a template engine, to HTML generation.
+ imake, a make interface using the C preprocessor, written for the X Window System but now deprecated in favour of automake.
+ grompp, a preprocessor for simulation input files for GROMACS (a fast, free, open-source code for some problems in computational chemistry) which calls the system C preprocessor (or other preprocessor as determined by the simulation input file) to parse the topology, using mostly the #define and #include mechanisms to determine the effective topology at grompp run time.
+ using GPP for preprocessing markdown files[5]

#
-

# Execution
Execution in computer and software engineering is __the process by which a computer or virtual machine executes the instructions of a computer program.__ Each instruction of a program is a description of a specific action to be carried out in order for a specific problem to be solved; as instructions of a program and therefore the actions they describe are being carried out by an executing machine, specific effects are produced in accordance to the semantics of the instructions being executed.

### Executable
In computing, executable code or an executable file or executable program, sometimes simply referred to as an executable or binary, causes a computer "to perform indicated tasks according to encoded instructions," as opposed to a data file that must be parsed by a program to be meaningful.

## Context of execution
The context in which execution takes place is crucial. Very few programs execute on a bare machine. Programs usually contain implicit and explicit assumptions about resources available at the time of execution. Most programs execute with the support of an operating system and run-time libraries specific to the source language that provide crucial services not supplied directly by the computer itself. This supportive environment, for instance, usually decouples a program from direct manipulation of the computer peripherals, providing more general, abstract services instead.

## Process
Prior to execution, a program must first be written. This is generally done in source code, which is then compiled at compile time (and statically linked at link time) to an executable. This executable is then invoked, most often by an operating system, which loads the program into memory (load time), possibly performs dynamic linking, and then begins execution by moving control to the entry point of the program; all these steps depend on the Application Binary Interface of the operating system. At this point execution begins and the program enters run time. The program then runs until it ends, either normal termination or a crash.

### Thread
In computer science, a thread of execution is __the smallest sequence of programmed instructions that can be managed independently by a scheduler__, which is typically a part of the operating system. The implementation of threads and processes differs between operating systems, but in most cases a thread is a component of a process. Multiple threads can exist within one process, executing concurrently and sharing resources such as memory, while different processes do not share these resources. In particular, the threads of a process share its executable code and the values of its dynamically allocated variables and non-thread-local global variables at any given time.

## Interpreter
__A system that executes a program__ is called an interpreter of the program. Loosely speaking, an interpreter actually does what the program says to do. This contrasts with a language translator that converts a program from one language to another. The most common language translators are compilers. Translators typically convert their source from a high-level, human readable language into a lower-level language (sometimes as low as native machine code) that is simpler and faster for the processor to directly execute. The idea is that the ratio of executions to translations of a program will be large; that is, a program need only be compiled once and can be run any number of times. This can provide a large benefit for translation versus direct interpretation of the source language. One trade-off is that development time is increased, because of the compilation. In some cases, only the changed files must be recompiled. Then the executable needs to be relinked. For some changes, the executable must be rebuilt from scratch. As computers and compilers become faster, this fact becomes less of an obstacle. Also, the speed of the end product is typically more important to the user than the development time.

### Bytecode interpreters
There is __a spectrum of possibilities between interpreting and compiling, depending on the amount of analysis performed before the program is executed.__ For example, Emacs Lisp is compiled to bytecode, which is a highly compressed and optimized representation of the Lisp source, but is not machine code (and therefore not tied to any particular hardware). This "compiled" code is then interpreted by a bytecode interpreter (itself written in C). The compiled code in this case is machine code for a virtual machine, which is implemented not in hardware, but in the bytecode interpreter. Such compiling interpreters are sometimes also called compreters. In a bytecode interpreter each instruction starts with a byte, and therefore bytecode interpreters have up to 256 instructions, although not all may be used. Some bytecodes may take multiple bytes, and may be arbitrarily complicated.

### Threaded code interpreters
Threaded code interpreters are __similar to bytecode interpreters but instead of bytes they use pointers.__ Each "instruction" is a word that points to a function or an instruction sequence, possibly followed by a parameter. The threaded code interpreter either loops fetching instructions and calling the functions they point to, or fetches the first instruction and jumps to it, and every instruction sequence ends with a fetch and jump to the next instruction. Unlike bytecode there is no effective limit on the number of different instructions other than available memory and address space. The classic example of threaded code is the Forth code used in Open Firmware systems: the source language is compiled into "F code" (a bytecode), which is then interpreted by a virtual machine.

### Abstract syntax tree interpreters
In the spectrum between interpreting and compiling, another approach is to __transform the source code into an optimized abstract syntax tree (AST),__ then execute the program following this tree structure, or use it to generate native code just-in-time. In this approach, each sentence needs to be parsed just once. As an advantage over bytecode, the AST keeps the global program structure and relations between statements (which is lost in a bytecode representation), and when compressed provides a more compact representation. Thus, using AST has been proposed as a better intermediate format for just-in-time compilers than bytecode. Also, it allows the system to perform better analysis during runtime.

### Just-in-time compilation
Further blurring the distinction between interpreters, bytecode interpreters and compilation is just-in-time compilation (JIT), __a technique in which the intermediate representation is compiled to native machine code at runtime.__ This confers the efficiency of running native code, at the cost of startup time and increased memory use when the bytecode or AST is first compiled. Adaptive optimization is a complementary technique in which the interpreter profiles the running program and compiles its most frequently executed parts into native code. Both techniques are a few decades old, appearing in languages such as Smalltalk in the 1980s.

### Self-interpreter
A self-interpreter is a programming language interpreter written in __a programming language which can interpret itself;__ an example is a BASIC interpreter written in BASIC. Self-interpreters are related to self-hosting compilers.

### Microcode
Microcode is a very commonly used technique "that imposes an interpreter between the hardware and the architectural level of a computer". As such, the microcode is __a layer of hardware-level instructions that implement higher-level machine code instructions or internal state machine sequencing in many digital processing elements.__ Microcode is used in general-purpose central processing units, as well as in more specialized processors such as microcontrollers, digital signal processors, channel controllers, disk controllers, network interface controllers, network processors, graphics processing units, and in other hardware.

# Inter-process Communication
In computer science, inter-process communication or interprocess communication (IPC) refers specifically to the mechanisms an operating system provides to __allow the processes to manage shared data__. Typically, applications can use IPC, categorized as clients and servers, where the client requests data and the server responds to client requests. Many applications are both clients and servers, as commonly seen in distributed computing. Methods for doing IPC are divided into categories which vary based on software requirements, such as performance and modularity requirements, and system circumstances, such as network bandwidth and latency.

IPC is very important to the design process for microkernels and nanokernels. Microkernels reduce the number of functionalities provided by the kernel. Those functionalities are then obtained by communicating with servers via IPC, increasing drastically the number of IPC compared to a regular monolithic kernel.

## Approaches

### File
A record stored on disk, or a record synthesized on demand by a file server, which can be accessed by multiple processes.

### Signal
A system message sent from one process to another, not usually used to transfer data but instead used to remotely command the partnered process.

### Socket
Data sent over a network interface, either to a different process on the same computer or to another computer on the network. Stream-oriented (TCP; data written through a socket requires formatting to preserve message boundaries) or more rarely message-oriented (UDP, SCTP).

### Unix domain socket
Similar to an internet socket but all communication occurs within the kernel. Domain sockets use the file system as their address space. Processes reference a domain socket as an inode, and multiple processes can communicate with one socket

### Message queue
A data stream similar to a socket, but which usually preserves message boundaries. Typically implemented by the operating system, they allow multiple processes to read and write to the message queue without being directly connected to each other.

### Pipe
A unidirectional data channel. Data written to the write end of the pipe is buffered by the operating system until it is read from the read end of the pipe. Two-way data streams between processes can be achieved by creating two pipes utilizing standard input and output.

### Named pipe
A pipe implemented through a file on the file system instead of standard input and output. Multiple processes can read and write to the file as a buffer for IPC data.

### Shared memory
Multiple processes are given access to the same block of memory which creates a shared buffer for the processes to communicate with each other.

### Message passing
Allows multiple programs to communicate using message queues and/or non-OS managed channels, commonly used in concurrency models.

### Memory-mapped file
A file mapped to RAM and can be modified by changing memory addresses directly instead of outputting to a stream. This shares the same benefits as a standard file.

## Application

### Remote procedure call interfaces
In distributed computing, a remote procedure call (RPC) is when a computer program causes a procedure (subroutine) to execute in a different address space (commonly on another computer on a shared network), which is coded as if it were a normal (local) procedure call, without the programmer explicitly coding the details for the remote interaction. That is, the programmer writes essentially the same code whether the subroutine is local to the executing program, or remote.[1] This is a form of client–server interaction (caller is client, executor is server), typically implemented via a request–response message-passing system. In the object-oriented programming paradigm, RPC calls are represented by remote method invocation (RMI). The RPC model implies a level of location transparency, namely that calling procedures is largely the same whether it is local or remote, but usually they are not identical, so local calls can be distinguished from remote calls. Remote calls are usually orders of magnitude slower and less reliable than local calls, so distinguishing them is important.

### Platform communication stack
utilize IPC mechanisms, but don't implement IPC themselves.

### Operating system communication stack
platform or programming language-specific APIs.

### Distributed object models
platform or programming language specific-APIs that use IPC, but do not themselves implement it.

# Runtime System
A runtime system, also called run-time system, primarily __implements portions of an execution model.__ This is in contrast to the runtime lifecycle phase of a program, during which the runtime system is in operation. Most languages have some form of runtime system, which implements control over the order in which work that was specified in terms of the language gets performed. Over the years, the meaning of the term 'runtime system' has been expanded to include nearly any behaviors that are dynamically determined during execution.

## Implementation details
When a program is to be executed, a __loader__ first performs the necessary memory setup and __links__ the program with any dynamically linked libraries it needs, and then the execution begins starting from the program's entry point. In some cases, a language or implementation will have these tasks done by the language runtime instead, though this is unusual in mainstream languages on common consumer operating systems.

## Runtime library
In computer programming, a runtime library (RTL) is __a set of low-level routines used by a compiler to invoke some of the behaviors of a runtime environment, by inserting calls to the runtime library into compiled executable binary.__ The runtime environment implements the execution model, built-in functions, and other fundamental behaviors of a programming language. During execution (run time) of that computer program, execution of those calls to the runtime library cause communication between the executable binary and the runtime environment. A runtime library often includes built-in functions for memory management or exception handling. Therefore, a runtime library is always specific to the platform and compiler.

# Virtual Machine
In computing, a virtual machine (VM) is an emulation of a computer system. Virtual machines are based on computer architectures and provide functionality of a physical computer. Their implementations may involve specialized hardware, software, or a combination.

## System Virtual Machines
The desire to run multiple operating systems was the initial motive for virtual machines, so as to allow time-sharing among several single-tasking operating systems. In some respects, a system virtual machine can be considered a generalization of the concept of virtual memory that historically preceded it. IBM's CP/CMS, the first systems to allow full virtualization, implemented time sharing by providing each user with a single-user operating system, the Conversational Monitor System (CMS). Unlike virtual memory, a system virtual machine entitled the user to write privileged instructions in their code. This approach had certain advantages, such as adding input/output devices not allowed by the standard system.

## Process Virtual Machines
A process VM, sometimes called an application virtual machine, or Managed Runtime Environment (MRE), runs as __a normal application inside a host OS and supports a single process. It is created when that process is started and destroyed when it exits.__ Its purpose is to provide a platform-independent programming environment that abstracts away details of the underlying hardware or operating system and allows a program to execute in the same way on any platform.

## Full virtualization
In full virtualization, the virtual machine simulates enough hardware to __allow an unmodified "guest" OS (one designed for the same instruction set) to be run in isolation.__ This approach was pioneered in 1966 with the IBM CP-40 and CP-67, predecessors of the VM family.

### Hardware-assisted virtualization
In hardware-assisted virtualization, the hardware __provides architectural support that facilitates building a virtual machine monitor and allows guest OSes to be run in isolation.__ Hardware-assisted virtualization was first introduced on the IBM System/370 in 1972,[citation needed] for use with VM/370, the first virtual machine operating system offered by IBM as an official product.

### Operating-system-level virtualization
In operating-system-level virtualization, __a physical server is virtualized at the operating system level, enabling multiple isolated and secure virtualized servers to run on a single physical server.__ The "guest" operating system environments share the same running instance of the operating system as the host system. Thus, the same operating system kernel is also used to implement the "guest" environments, and applications running in a given "guest" environment view it as a stand-alone system. The pioneer implementation was FreeBSD jails; other examples include Docker, Solaris Containers, OpenVZ, Linux-VServer, LXC, AIX Workload Partitions, Parallels Virtuozzo Containers, and iCore Virtual Accounts.

